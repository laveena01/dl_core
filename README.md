#  From-Scratch Implementation of Core Machine Learning Algorithms


**Tags:** Machine Learning, Optimization, Gradient Descent, Perceptron, MLP

---

## Overview

The goal is to **understand the mathematical foundations** of optimization, loss functions, and convergence behavior through hands-on experimentation and visualization.

---
##  Core Features

* Manual implementation of **forward pass**, **gradient computation**, and **parameter updates**.
* Support for **early stopping**, **L2 regularization**, and **custom learning rate scheduling**.
* **Visualization tools** for decision boundaries, loss convergence, and noise robustness.
* Comparative analysis between **MSE vs MAE**, **random vs zero initialization**, and **loss sensitivity to noise**.

---

## Visualizations

* Decision boundary evolution across epochs.
* Loss convergence plots (train vs test).
* Effect of noise and regularization on model stability.

---

